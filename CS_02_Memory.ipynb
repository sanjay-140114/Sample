{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjay-140114/Sample/blob/master/CS_02_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "S419ONNx4oKU"
      },
      "id": "S419ONNx4oKU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip install --upgrade openai\n",
        "from openai import OpenAI\n",
        "#from openai import AsyncOpenAI\n",
        "import os\n",
        "import json\n",
        "!pip install  python-dotenv\n",
        "!pip install  langchain"
      ],
      "metadata": {
        "id": "UDdhBY6GlHYT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "UDdhBY6GlHYT"
    },
    {
      "cell_type": "markdown",
      "id": "a786c77c",
      "metadata": {
        "id": "a786c77c"
      },
      "source": [
        "# **LangChain:** Memory\n",
        "\n",
        "## Outline\n",
        "* ConversationBufferMemory\n",
        "* ConversationBufferWindowMemory\n",
        "* ConversationTokenBufferMemory\n",
        "* ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1297dcd5",
      "metadata": {
        "id": "1297dcd5"
      },
      "source": [
        "## ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1f518f5",
      "metadata": {
        "height": 149,
        "id": "a1f518f5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg7tZF3_6gg1",
        "outputId": "e1171a2c-5277-40d0-f581-18bef557c7f8"
      },
      "id": "pg7tZF3_6gg1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.352-py3-none-any.whl (794 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/794.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m614.4/794.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.4/794.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain)\n",
            "  Downloading langchain_community-0.0.6-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1 (from langchain)\n",
            "  Downloading langchain_core-0.1.3-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.70 (from langchain)\n",
            "  Downloading langsmith-0.0.75-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.352 langchain-community-0.0.6 langchain-core-0.1.3 langsmith-0.0.75 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ad6fe2",
      "metadata": {
        "height": 98,
        "id": "20ad6fe2"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88bdf13d",
      "metadata": {
        "height": 132,
        "id": "88bdf13d"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0.0)\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db24677d",
      "metadata": {
        "height": 30,
        "id": "db24677d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "7d950058-f9a3-4c35-cbeb-5c2043b3c9b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Andrew\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Andrew! It's nice to meet you. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Andrew\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3ef937",
      "metadata": {
        "height": 30,
        "id": "cc3ef937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "c7c1865e-4bc7-400d-8634-391753e4b722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1+1 is equal to 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf3339a",
      "metadata": {
        "height": 30,
        "id": "acf3339a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "24da0a1a-5c5f-4542-e5b5-93fa52425ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 is equal to 2.\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2529400d",
      "metadata": {
        "height": 30,
        "id": "2529400d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd7a276-beb6-43c8-c6a3-b9c1889c7cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: What is 1+1?\n",
            "AI: 1+1 is equal to 2.\n",
            "Human: What is my name?\n",
            "AI: I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5018cb0a",
      "metadata": {
        "height": 30,
        "id": "5018cb0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "892c951d-3ce1-428e-efde-1cc8e551a05a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14219b70",
      "metadata": {
        "height": 30,
        "id": "14219b70"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a36e9905",
      "metadata": {
        "height": 47,
        "id": "a36e9905"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61631b1f",
      "metadata": {
        "height": 30,
        "id": "61631b1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc86c0d-2b5b-44e7-8f03-89cffdfae3e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi\n",
            "AI: What's up\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2fdf9ec",
      "metadata": {
        "height": 30,
        "id": "a2fdf9ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab94a96-cb59-4a2f-e11b-24422c08a841"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\"}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ca79256",
      "metadata": {
        "height": 64,
        "id": "7ca79256"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890a4497",
      "metadata": {
        "height": 30,
        "id": "890a4497",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8306e4-d939-4b19-b85d-0cb0ca920f7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\"}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf98e9ff",
      "metadata": {
        "id": "cf98e9ff"
      },
      "source": [
        "## ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66eeccc3",
      "metadata": {
        "height": 47,
        "id": "66eeccc3"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea6233e",
      "metadata": {
        "height": 47,
        "id": "3ea6233e"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc4553fb",
      "metadata": {
        "height": 115,
        "id": "dc4553fb"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a788403",
      "metadata": {
        "height": 30,
        "id": "6a788403",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd00ac7c-ad2b-4b3c-924c-a66564383024"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4087bc87",
      "metadata": {
        "height": 132,
        "id": "4087bc87"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0.0)\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4faaa952",
      "metadata": {
        "height": 30,
        "id": "4faaa952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5db00070-8549-46b4-e450-5a535dfbde75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Andrew! It's nice to meet you. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Andrew\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb20ddaa",
      "metadata": {
        "height": 30,
        "id": "bb20ddaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "73a32e34-8521-4963-9594-b79205b98169"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1+1 is equal to 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "489b2194",
      "metadata": {
        "height": 30,
        "id": "489b2194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e22ea99d-42e6-4d48-e544-230671f3ee0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2931b92",
      "metadata": {
        "id": "d2931b92"
      },
      "source": [
        "## ConversationTokenBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6d063c",
      "metadata": {
        "height": 30,
        "id": "9f6d063c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4436d95-698b-49ca-b493-2528804b603f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9020ed",
      "metadata": {
        "height": 81,
        "id": "fb9020ed"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "llm = ChatOpenAI(temperature=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43582ee6",
      "metadata": {
        "height": 149,
        "id": "43582ee6"
      },
      "outputs": [],
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n",
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "284288e1",
      "metadata": {
        "height": 30,
        "id": "284288e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f3ca71-fbfd-4ac3-cea0-dc985c3d83e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff55d5d",
      "metadata": {
        "id": "5ff55d5d"
      },
      "source": [
        "## ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72dcf8b1",
      "metadata": {
        "height": 64,
        "id": "72dcf8b1"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a5b238f",
      "metadata": {
        "height": 285,
        "id": "4a5b238f"
      },
      "outputs": [],
      "source": [
        "# create a long string\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e4ecabe",
      "metadata": {
        "height": 30,
        "id": "2e4ecabe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4badbab-e370-4aa5-8bda-39a957c1fb07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI also mentions the need to bring a laptop to showcase the latest LLM demo during the lunch meeting.'}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6728edba",
      "metadata": {
        "height": 98,
        "id": "6728edba"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a221b1d",
      "metadata": {
        "height": 47,
        "id": "9a221b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "35981aa0-dbd2-4d2c-98ce-74c5f3bdb73b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI also mentions the need to bring a laptop to showcase the latest LLM demo during the lunch meeting.\n",
            "Human: What would be a good demo to show?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A good demo to show during the lunch meeting with the customer interested in AI would be the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. It has been trained on a vast amount of data and can be used for various applications such as content generation, chatbots, and language translation. By showcasing the LLM demo, you can demonstrate the capabilities and potential of AI in a practical and engaging way.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb582617",
      "metadata": {
        "height": 30,
        "id": "bb582617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a157405b-aeeb-4b5c-ea2d-42ba1a77b2bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'System: The human and AI exchange greetings and discuss the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI also mentions the need to bring a laptop to showcase the latest LLM demo during the lunch meeting. The human asks what would be a good demo to show, and the AI suggests showcasing the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. It has been trained on a vast amount of data and can be used for various applications such as content generation, chatbots, and language translation. By showcasing the LLM demo, the AI believes the human can demonstrate the capabilities and potential of AI in a practical and engaging way.'}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ba827aa",
      "metadata": {
        "height": 30,
        "id": "4ba827aa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c07922b",
      "metadata": {
        "height": 30,
        "id": "2c07922b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52696c8c",
      "metadata": {
        "height": 30,
        "id": "52696c8c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48690d13",
      "metadata": {
        "height": 30,
        "id": "48690d13"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85bba1f8",
      "metadata": {
        "height": 30,
        "id": "85bba1f8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a62e89",
      "metadata": {
        "height": 30,
        "id": "96a62e89"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f953ad8d",
      "metadata": {
        "height": 30,
        "id": "f953ad8d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9bb2617",
      "metadata": {
        "height": 30,
        "id": "b9bb2617"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be1989bf",
      "metadata": {
        "height": 30,
        "id": "be1989bf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1f8dca6",
      "metadata": {
        "height": 30,
        "id": "b1f8dca6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9120949",
      "metadata": {
        "height": 30,
        "id": "e9120949"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55849fb5",
      "metadata": {
        "height": 30,
        "id": "55849fb5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ae491b",
      "metadata": {
        "height": 30,
        "id": "94ae491b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c3aec3",
      "metadata": {
        "height": 30,
        "id": "e7c3aec3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae51a3f",
      "metadata": {
        "height": 30,
        "id": "5ae51a3f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06894001",
      "metadata": {
        "height": 30,
        "id": "06894001"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2fb674c",
      "metadata": {
        "height": 30,
        "id": "e2fb674c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5158f4c8",
      "metadata": {
        "height": 30,
        "id": "5158f4c8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cebe066",
      "metadata": {
        "height": 30,
        "id": "2cebe066"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65caee4d",
      "metadata": {
        "height": 30,
        "id": "65caee4d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04799230",
      "metadata": {
        "height": 30,
        "id": "04799230"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30795c10",
      "metadata": {
        "height": 30,
        "id": "30795c10"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ed59c4",
      "metadata": {
        "height": 30,
        "id": "08ed59c4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb43eee3",
      "metadata": {
        "height": 30,
        "id": "bb43eee3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}